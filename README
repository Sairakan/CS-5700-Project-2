Seung Son: 001894138

Jason Teng: 001876580


Jason Teng worked on the http parser, as well as the login logic. While developing the code for logging in, we discovered the requirement for a CSRF token. We opted to grab the CSRF token directly from the response header, although it could also have been obtained via the hidden div in the html response body. As for the http request code, we had little difficulty implementing the code to handle error responses. Ultimately, the most difficulty was encountered with regards to handling chunked data in the response. Initially, our parser assumed that all chunks would be sent in separate packets, which led to errors when the server response put the 0 length chunk at the end of the original response. Once this was fixed, however, the http request/response handler worked without issue.

Seung Son worked on the implementation of the web crawler, starting from the search of links within the response's raw html to laying the foundations for looping through Fakebook in order to find the secret flags. The implementation of the link-searching was relatively smooth, since we used a simple list as our queue of links and checked new URL found while crawling with the elements of the list for duplicates before adding them in. The web crawler is set to crawl through only Fakebook by checking if the URL begins with "/", showing that it is under the base URL of Fakebook. The bash file for the command of ./webcrawler [username] [password] was relatively simple. Using only one or no arguments will tell you that you are missing the required number of arguments. If you input the wrong username or password or have more than two arguments put in, the script will tell you that there is a mistake in your arguments.

There were quite a few bugs that appeared during the looping so we both worked together to fix these. Although there were other problems such as misformatted "GET" requests, the main problem appeared with HTTP 1.1's chunked transfer encoding, which made parsing difficult. 
